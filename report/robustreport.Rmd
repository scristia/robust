---
title: "Robustifying doubly-robust estimators"
author: "Stephen Cristiano"
date: "12/7/2017"
output:
  pdf_document:
    latex_engine: xelatex
    citation_package: biblatex
fontsize: 12pt
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
Consider the following setting:

* We wish to estimate the mean of $Y$, an outcome of interest, given observed covariates $X$.
* Incomplete data: $R \in {0,1}$ is an indicator, where $R=0$ when $Y$ is missing
* We assume $P(R=1|Y,X) = P(R=1|X) = P(X)$, i.e. missing at random.


Recall a doubly robust estimator for the mean, $\mu$, has the form:

$$\hat{\mu}_{dr} = \frac{1}{n} \sum_{i=1}^n \left\{ \frac{R_i Y_i}{\pi(X_i, \hat{\gamma})} - \frac{R_i -  \pi(X_i, \hat{\gamma})}{\pi(X_i, \hat{\gamma})} m(X, \hat{\beta}) \right\}$$

Where $\hat{\beta}$ is estimated via complete cases regression, i.e. condition on $R=1$,
$\pi(X_i, \hat{\gamma})$ are the propensity scores estimated via logistic regression,
and $m(X,\hat{\beta})$ is the estimated mean of the regression. Doubly robust estimators have
the property where if either the outcome regression model or the propensity score model is misspecified
(but not both), $\hat{\mu}_{dr}$ is still a consistent estimator for $\mu$.

When outliers are present and also dependent on the distribution of $X$,
doubly robust estimators become very unstable with regards to MSE. In particular,
inverse probability weighting by the propensity score poses a problem when outliers exist,
as it can lead to values close to zero in the denominator and hence unreliable estimation.
Our goal is to modify the doubly robust estimator by weighting by some function of the
influence each observation has on the overall estimates to mitigate the the effect
outlying samples may have.

A common approach in robust regression settings for parameter estimation while
mitigating the effect of outliers is to define a score function in which
influential observations are downweighted with regards to some weighting function.
An iterated re-weighted least squares algorithm is used to iteratively update the weights
and estimates for parameters of interest until some convergence rule.

## Model

To robustly solve for $\mu$, we set up a vector of weighted of estimating equations:
$$
U(Y,X; \gamma, \beta, \mu, w) =
    \begin{bmatrix}
        u_1(R, Y, X; \gamma, \beta, \mu, w) \\
        u_2(R, X; \gamma, w) \\
        u_3(R, Y, X; \beta, w)
    \end{bmatrix}
$$
Where $u_1, u_2, u_3$ are the estimating equations for $\mu, \gamma$, and $\beta$, respectively.

Our goal is to iteratively solve
$$
\begin{aligned}
   u_1 &= \sum_i w_i \left( \frac{RY}{\pi(x_i, \gamma)} - \frac{R - \pi(x_i, \gamma)}{R - \pi(x_i, \gamma)} m(X\beta) - \mu \right) = 0 \\
   u_2 &= \sum_i w_i \left(R_i - \pi(x_i, \gamma) \right)x_i = 0 \\
   u_3 &= \sum_i w_i \left(Y_i - m(x_i, \beta) \right)x_i = 0
\end{aligned}
$$


Define
$$
\hat{A} = \frac{1}{n} \sum_{i=1}^{n} U_i(y_i, x_i; \theta)U_i^t(y_i, x_i; \theta).
$$
For notational convenience, let $\theta = (\gamma, \beta, \mu)$. Define the norm $\lVert \cdot \rVert_M$ of a $p \times p$ positive definite matrix $M$ on $\mathbb{R}^p$ by
$$ \lVert x \rVert_M = [x^t M^{-1} x]^{1/2}
$$

Let $\psi$ be Hampel's weight function, defined as

$$
\psi(u) = \begin{cases} 
      1 & \text{if } |u| < a \\
      \frac{a}{|u|} & \text{if } a\leq |u| < b \\
      a\frac{c/|u| - 1}{c-b} & \text{if } b\leq |u| < c \\
      0 & \text{otherwise}
\end{cases}
$$

We apply $\psi$ with $a=2,b=4,c=8$ to weight each observation according to its influence with the rule
$$
    w_i = \psi\left( \frac{\lVert U_i \rVert_{\hat{A}}}{1.48\times\text{median}({\lVert U_i \rVert_{\hat{A}}})}\right)
$$

Using an iterated reweighted least squares approach for estimation, our algorithm for solving
$\hat{\theta}$ is as follows:

***
1. Fix $\epsilon = 0.0001$. Let $s=1$ be the current iteration. Set our starting values:
$$
  \begin{aligned}
    w_i^{(1)} &= 1 \text{ for all } i \\
    \tilde{\beta}^{(1)} &= (X_{R=1}^t X_{R=1})^{-1} X_{R=1}^t Y_{R=1} \\
    \tilde{\gamma}^{(1)} &=\text{coefficents of logistic regression} \\
    \tilde{\mu}^{(1)} &= \frac{1}{n}\sum_{i=1}^n \left(\frac{RY}{\pi(x_i, \tilde{\gamma}^{(1)})} - \frac{R - \pi(x_i, \tilde{\gamma}^{(1)})}{R - \pi(x_i, \tilde{\gamma}^{(1)})} m(X\tilde{\beta}^{(1)})\right) \\
  \end{aligned}
$$

2. Solve
   $$
   \hat{A}^{(s+1)} = \frac{1}{n} \sum_{i=1}^{n} U_i(y_i, x_i; \tilde{\theta}^{(s)})U_i^t(y_i, x_i; \tilde{\theta}^{(s)}).
   $$k
3. Update the weights:
$$
    w_i^{(s+1)} = \psi\left( \frac{\lVert U_i^{(s)} \rVert_{\hat{A}^{(s)}}}{a\times\text{median}({\lVert U_i^{(s)} \rVert_{\hat{A}^{(s)}}})}\right)
$$
Set $W^{(s+1)}$ to be the vector of $w_i^{(s+1)}$'s. If $w_i^{(s)} = 0$, then $w_i^{(s+1)} = 0$.

4. Solve the estimating equations. Our parameters become
$$
  \begin{aligned}
    \tilde{\beta}^{(s+1)} &=\text{coefficients of weighted least squares using} W^{(s+1)} \\
    \tilde{\gamma}^{(s+1)} &=\text{coefficents of weighted logistic regression using} W^{(s+1)} \\
    \tilde{\mu}^{(s+1)} &= \sum_{i=1}^n w_i^{(s+1)} \left(\frac{RY}{\pi(x_i, \tilde{\gamma}^{(s+1)})} - \frac{R - \pi(x_i, \tilde{\gamma}^{(s+1)})}{R - \pi(x_i, \tilde{\gamma}^{(s+1)})} m(X\tilde{\beta}^{(s+1)})\right)
  \end{aligned}
$$

5. If $\lvert \theta^{(s+1)} - \theta^{(s)} \rvert < \epsilon$, end. Else, return to step 2. 

***
To estimate the coefficiences from weighted regressions, we use the `glm` function in `R` with
the weight option set to $W$.

## Simulation

Closely follows the scenario proposed by Tsiatsis and Davidian 'More Robust Doubly Robust Estimators", but
with an additional step to randomly "corrupt" outcomes, creating outliers.

* $Z_i = (Z_{i1}, \ldots, Z_{i4})^t \sim N(0, 1)$ with $n=500$.
* $X_i = (X_{i1}, \ldots, X_{i4})^t$ where 
    * $X_{i1} = \exp(Z_{i1}/2)$
    * $X_{i2} = Z_{i2}/\{1 + \exp(Z_{i1})\} + 10$
    * $X_{i3} = (Z_{i1}Z_{i3}/25 + 0.6)^3$ and
    * $X_{i4} = (Z_{i3} + Z+{i4} + 20)^2$.
* Let the true outcome model be $Y|X \sim N(m_0(X), 1)$.
    * $m_0(X) = 210 + 24.7 Z_1 + 13.7 Z_2 + 13.7 Z_3 + 13.7 Z_4$
    * "Corrupt" 10\% of the $y_i$'s by simulating $y_i | x_i \sim N(m_0(x_i), 7)$ to create outliers.
* True propensity score model:
    * $\pi_0 = \text{expit}(-Z_1 + 0.5 Z_2 - 0.25 Z_3 - 0.1 Z_4)$
    * Misspecified models use $X's$ instead of $Z's$.
* True $\mu_0 = 210$. 

**Implementation**
```{r, echo=FALSE, cache=TRUE}
library(MASS)
expit <- function(x) exp(x)/(1 + exp(x))
robust.dr <- function(y, R, Z, X, mu.start, beta.start, gamma.start, correct="both", maxit=50, eps=0.0001) {
    mu.update <- mu.start
    beta.update <- beta.start
    gamma.update <- gamma.start
    ps <- expit(Z %*% gamma)

    if(correct=="OR" | correct=="both") mu.or <- Z %*% beta
    if(correct=="PS" | correct == "none") {
        mu.or <- X %*% beta
    }

    for(i in 1:maxit) {
        ee.u1 <- function(y, R, mu.or, ps, w, mu) {
            w*(R*y/ps - (R - ps)/ps * mu.or - mu)
        }
        u1 <- ee.u1(y, R, mu.or, ps, w, mu.update)

        ee.u2 <- function(R, Z, ps, w) {
            apply(Z, 2, function(z) w * (R - ps)*z)
        }
        if(correct=="PS" | correct=="both") u2 <- ee.u2(R, Z, ps, w)

        if(correct=="OR" | correct == "none") {
            u2 <- ee.u2(R, X, ps, w)
        }

        ee.u3 <- function(y, R, Z, beta, w) {
            apply(Z, 2, function(z) w * R * (y - Z %*% beta)*z)
        }

        if(correct=="OR" | correct == "both") u3 <- ee.u3(y, R, Z, beta, w) 
        if(correct=="PS" | correct == "none") {
            u3 <- ee.u3(y, R, X, beta, w)
        }


        A.list <- lapply(1:1000, function(x) {
                             U <- c(u1[x],  u2[x,], u3[x,])
                             U %*% t(U)
        })
        A <- Reduce("+", A.list) / length(A.list)

        ### A singular when OR and PS models both incorrect.
        U <- cbind(u1, u2, u3)
        findWeights <- function(U, A, a) {
#             norm <-  apply(U, 1, function(u) sqrt(t(u) %*% solve(A) %*% u))
            norm <-  apply(U, 1, function(u) sqrt(t(u) %*% chol2inv(chol(A)) %*% u))
            w0 <- a * sqrt(ncol(A)) / norm
            ifelse(w0 > 1, 1, w0)
            w <- psi.hampel(norm/(a*median(norm)))

            ## extreme outliers get weight 0 on odd iterations and weight 1 on even.
            ## fix so if an observation gets 0 weight, it always keeps that weight.
            w[which(norm == 0)] <- 0
            w
        }
        w <- findWeights(U, A, a)


        theta.prev <- c(beta.update, gamma.update, mu.update)
        if(correct=="OR"|correct=="both") {
            beta.update <- lm(y[R==1] ~ Z[R==1,-1], weights=w[R==1])$coeff
            mu.or <- Z %*% beta.update
        }

        if(correct=="PS"|correct=="none") {
            beta.update <- lm(y[R==1] ~ X[R==1,-1], weights=w[R==1])$coeff
            mu.or <- X %*% beta.update
        }

        if(correct=="PS"|correct=="both") {
            gamma.update <- coef(glm(R ~ Z[,-1], family="binomial", weights=w))
            ps <- expit(Z %*% gamma.update)
        }
        if(correct=="OR"|correct=="none") {
            gamma.update <- coef(glm(R ~ X[,-1], family="binomial", weights=w))
            ps <- expit(X %*% gamma.update)
        }

        mu.update <- weighted.mean(R*y/ps - (R - ps)/ps * mu.or, w)
        theta <- c(beta.update, gamma.update, mu.update)

        converged <- sqrt(sum(((theta - theta.prev)^2)))/sqrt(sum(((theta)^2))) < eps
        if(converged) break
    }
    mu.update
}
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
mu.both.correct  <- rep(NA, 500)
mu.ps.correct <- rep(NA, 500)
mu.or.correct <-  rep(NA, 500)
mu.both.incorrect <-  rep(NA, 500)
mu <- rep(NA, 500)

for(i in 1:500) {
    set.seed(i)
    Z <- cbind(1, matrix(rnorm(1000*4), nrow=1000, ncol=4))

    x1 <- exp(Z[,2]/2)
    x2 <- Z[,3]/(1 + exp(Z[,2])) + 10
    x3 <- (Z[,2] * Z[,4] / 25 + 0.6)^3
    x4 <- (Z[,4] + Z[,5] + 20)^2

    X <- cbind(1, x1, x2, x3, x4)

    ### True outcome
#     y <- Z %*% c(210, 27.4, 13.7, 13.7, 13.7) + rnorm(1000, 0, 1)
    ###   ^^ no outliers ^^ -- vv outliers vv
    y <- Z %*% c(210, 27.4, 13.7, 13.7, 13.7)
    y[1:100] <- y[1:100] + rnorm(100, 0, 7^2)
    y[101:1000] <- y[101:1000] + rnorm(900, 0, 1)

    expit <- function(x) exp(x)/(1 + exp(x))
    pi0 <- expit(-Z[,2] + 0.5*Z[,3] - 0.25*Z[,4] - 0.1*Z[,5])

    R <- rbinom(1000, 1, pi0)
    ##### Startings values
    w <- rep(1, length(y))
    beta <- lm(y[R==1] ~ Z[R==1,-1], weights=w[R==1])$coeff
    gamma <- coef(glm(R ~ Z[,-1], family="binomial"), weights=w)
    ps <- expit(Z %*% gamma)
    mu <- weighted.mean(R*y/ps - (R - ps)/ps * Z %*% beta, w)
    a <- 1.48

    beta.start <- beta
    gamma.start <- gamma
    ps.start <- ps
    mu.start <- mu

    eps = 0.0001
    maxit=16
    ###################

    mu.both.correct[i] <- robust.dr(y, R, Z, X, mu.start, beta.start, gamma.start, correct="both", maxit=maxit, eps=0.001)
    mu.ps.correct[i] <- robust.dr(y, R, Z, X, mu.start, beta.start, gamma.start, correct="PS", maxit=maxit, eps=0.001)
    mu.or.correct[i] <- robust.dr(y, R, Z, X, mu.start, beta.start, gamma.start, correct="OR", maxit=maxit, eps=0.001)
    mu.both.incorrect[i] <- robust.dr(y, R, Z, X, mu.start, beta.start, gamma.start, correct="none", maxit=maxit, eps=0.001)
}
```

```{r, echo=FALSE}
# mse <- bias^2 + var(mu.or.correct[-484])
bias.both.correct <- 210 - mean(mu.both.correct)
mse.both.correct <- bias.both.correct^2 + var(mu.both.correct)
# bias.both.correct
# sqrt(mse.both.correct)

bias.ps.correct <- 210 - mean(mu.ps.correct)
mse.ps.correct <- bias.ps.correct^2 + var(mu.ps.correct)
# bias.ps.correct
# sqrt(mse.ps.correct)

bias.or.correct <- 210 - mean(mu.or.correct)
mse.or.correct <- bias.or.correct^2 + var(mu.or.correct)
# bias.or.correct
# sqrt(mse.or.correct)

bias.both.incorrect <- 210 - mean(mu.both.incorrect)
mse.both.incorrect <- bias.both.incorrect^2 + var(mu.both.incorrect)
# bias.both.incorrect
# sqrt(mse.both.incorrect)

bias <- c(bias.both.correct, bias.ps.correct, bias.or.correct, bias.both.incorrect)
rmse <- sqrt(c(mse.both.correct, mse.ps.correct, mse.or.correct, mse.both.incorrect))

tab <- cbind(bias, rmse)
rownames(tab) <- c("Both Correct", "OR Wrong", "PS Wrong", "Both Wrong")
colnames(tab) <- c("Bias", "RMSE")
```

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
mu.dr <- rep(NA, 500)
mu.or.wrong <- rep(NA, 500)
mu.ps.wrong <- rep(NA, 500)
mu.both.wrong <- rep(NA, 500)

for(i in 1:500) {
    set.seed(i)
    Z <- cbind(1, matrix(rnorm(1000*4), nrow=1000, ncol=4))

    x1 <- exp(Z[,2]/2)
    x2 <- Z[,3]/(1 + exp(Z[,2])) + 10
    x3 <- (Z[,2] * Z[,4] / 25 + 0.6)^3
    x4 <- (Z[,4] + Z[,5] + 20)^2

    X <- cbind(1, x1, x2, x3, x4)

    ### True outcome
#     y <- Z %*% c(210, 27.4, 13.7, 13.7, 13.7) + rnorm(1000, 0, 1)
    ###   ^^ no outliers ^^ -- vv outliers vv
    y <- Z %*% c(210, 27.4, 13.7, 13.7, 13.7)
    y[1:100] <- y[1:100] + rnorm(100, 0, 7^2)
    y[101:1000] <- y[101:1000] + rnorm(900, 0, 1)

    expit <- function(x) exp(x)/(1 + exp(x))
    pi0 <- expit(-Z[,2] + 0.5*Z[,3] - 0.25*Z[,4] - 0.1*Z[,5])

    R <- rbinom(1000, 1, pi0)

    beta.h <- solve((t(Z) %*% Z)) %*% t(Z) %*% y
    ### complete cases (correct regression model)
    beta.or <- solve(t(Z[R==1,]) %*% Z[R==1,]) %*% t(Z[R==1,]) %*% y[R==1]
    ### complete cases (incorrect regression model)
    beta.or.wrong <- solve(t(X[R==1,]) %*% X[R==1,]) %*% t(X[R==1,]) %*% y[R==1]
    ### estimate propensity score
    gamma.h <- coef(glm(R ~ Z[,-1], family="binomial"))
    ps <- expit(Z %*% gamma.h)
    mu.ipw <- mean(R * y / ps)

    ### incorect propensity score
    ps.wrong <- expit(X %*% coef(glm(R ~ X[,-1], family="binomial")))

    #### Doubly Robust estimator
    mu.dr[i] <- mean(R*y/ps - (R - ps)/ps * Z %*% beta.or)
    mu.or.wrong[i] <- mean(R*y/ps - (R - ps)/ps * X %*% beta.or.wrong)
    mu.ps.wrong[i] <- mean(R*y/ps.wrong - (R - ps.wrong)/ps.wrong * Z %*% beta.or)
    mu.both.wrong[i] <- mean(R*y/ps.wrong - (R - ps.wrong)/ps.wrong * X %*% beta.or.wrong)
}

bias.both.correct2 <- 210 - mean(mu.dr)
mse.both.correct2 <- bias.both.correct2^2 + var(mu.dr)

bias.ps.correct2 <- 210 - mean(mu.or.wrong)
mse.ps.correct2 <- bias.ps.correct2^2 + var(mu.or.wrong)

bias.or.correct2 <- 210 - mean(mu.ps.wrong)
mse.or.correct2 <- bias.or.correct2^2 + var(mu.ps.wrong)

bias.both.incorrect2 <- 210 - mean(mu.both.wrong)
mse.both.incorrect2 <- bias.both.incorrect2^2 + var(mu.both.wrong)

bias2 <- c(bias.both.correct2, bias.ps.correct2, bias.or.correct2, bias.both.incorrect2)
rmse2 <- sqrt(c(mse.both.correct2, mse.ps.correct2, mse.or.correct2, mse.both.incorrect2))

tab2 <- cbind(bias2, rmse2)
rownames(tab2) <- c("Both Correct", "OR Wrong", "PS Wrong", "Both Wrong")
colnames(tab2) <- c("Bias", "RMSE")
```

From our simulations, we apply both the standard doubly robust estimator (DR) and our
re-weighted doubly robust estimator (RWDR). Evaluating the bias and root mean squared error
for the two approaches, we see:

```{r, echo=FALSE, warning=FALSE, cache=TRUE}
fulltab <- cbind(tab2, tab)
colnames(fulltab) <- c("Bias~DR~", "RMSE~DR~", "Bias~RWDR~", "RMSE~RWDR~")
kable(round(fulltab, 2))
```

Hence our reweighted approach greatly reduces the variance of the estimator in
the cases where the propensity score model is wrong, but seems to introduce small
amount of bias when both models are correct.
